{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Introduction to Hugging Face Transformers\n",
    "\n",
    "This notebook introduces Hugging Face's ecosystem and demonstrates how to:\n",
    "1. Set up the environment for local models\n",
    "2. Load your first model\n",
    "3. Perform basic inference\n",
    "4. Compare local models with API-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "First, let's install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Uncomment and run this cell if you need to install the libraries\n",
    "# !pip install torch transformers accelerate gradio python-dotenv\n",
    "# !pip install bitsandbytes  # Optional: for 4-bit quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Hugging Face's Ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def introduction_to_huggingface():\n",
    "    \"\"\"Print an introduction to Hugging Face's ecosystem\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTRODUCTION TO HUGGING FACE TRANSFORMERS\".center(80))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    "Hugging Face is an AI community and platform that provides:\n",
    "\n",
    "1. 🤗 Model Hub: A repository of pre-trained models (100,000+) for NLP, computer vision, \n",
    "   audio processing, and more.\n",
    "\n",
    "2. 🔧 Transformers Library: A Python library that provides APIs and tools to easily \n",
    "   download and train state-of-the-art pretrained models.\n",
    "\n",
    "3. 📚 Datasets: A library and platform for easily sharing and accessing datasets.\n",
    "\n",
    "4. 🧪 Spaces: A platform for hosting ML demo apps.\n",
    "\n",
    "5. 🧠 AutoTrain: Tools for training models without writing code.\n",
    "\n",
    "Key advantages of using Hugging Face for local models:\n",
    "- Run models on your own hardware without API costs\n",
    "- Full control over model parameters and behavior\n",
    "- Privacy - data doesn't leave your machine\n",
    "- Ability to fine-tune models for specific use cases\n",
    "- No internet connection required for inference\n",
    "    \"\"\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "introduction_to_huggingface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if CUDA is available\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Print GPU info if available\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading a Model\n",
    "\n",
    "Let's load a small model first. We'll use TinyLlama, which is a 1.1B parameter model that can run on most hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_model(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_4bit=True):\n",
    "    \"\"\"Load a model from Hugging Face Hub\"\"\"\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "    print(\"This may take a few moments depending on your internet connection and the model size...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Load model with quantization if requested\n",
    "        if use_4bit and DEVICE == \"cuda\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                load_in_4bit=True\n",
    "            )\n",
    "            print(\"Model loaded with 4-bit quantization\")\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "                torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "            )\n",
    "            print(f\"Model loaded in {'16-bit' if DEVICE == 'cuda' else '32-bit'} precision\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Check your internet connection\")\n",
    "        print(\"2. Verify the model name is correct\")\n",
    "        print(\"3. Try a smaller model if you're running out of memory\")\n",
    "        print(\"4. Make sure you have the latest transformers library\")\n",
    "        return None, None\n",
    "\n",
    "# Load a small model\n",
    "model, tokenizer = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Inference\n",
    "\n",
    "Now let's perform basic inference with our loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def basic_inference(model, tokenizer, prompt, max_length=512, temperature=0.7):\n",
    "    \"\"\"Perform basic inference with a loaded model\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return \"Model or tokenizer not loaded correctly.\"\n",
    "    \n",
    "    try:\n",
    "        # Format the prompt based on model type\n",
    "        if \"llama\" in model.config.architectures[0].lower():\n",
    "            # Format for Llama models\n",
    "            formatted_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "        elif \"mistral\" in model.config.architectures[0].lower():\n",
    "            # Format for Mistral models\n",
    "            formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "        elif \"phi\" in model.config.architectures[0].lower():\n",
    "            # Format for Phi models\n",
    "            formatted_prompt = f\"User: {prompt}\\nAssistant:\"\n",
    "        else:\n",
    "            # Default format\n",
    "            formatted_prompt = prompt\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Move inputs to the appropriate device\n",
    "        if DEVICE == \"cuda\":\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate text\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the assistant's response\n",
    "        if \"<|assistant|>\" in formatted_prompt:\n",
    "            response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "        elif \"[/INST]\" in formatted_prompt:\n",
    "            response = generated_text.split(\"[/INST]\")[-1].strip()\n",
    "        elif \"Assistant:\" in generated_text:\n",
    "            response = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "        else:\n",
    "            response = generated_text.replace(prompt, \"\").strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error during inference: {str(e)}\"\n",
    "\n",
    "# Test the model with a few prompts\n",
    "test_prompts = [\n",
    "    \"What are the main features of Python?\",\n",
    "    \"Write a short poem about artificial intelligence.\",\n",
    "    \"Explain quantum computing to a 10-year-old.\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n\\nPrompt {i+1}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = basic_inference(model, tokenizer, prompt)\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experimenting with Generation Parameters\n",
    "\n",
    "Let's see how different parameters affect the generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test with different temperatures\n",
    "prompt = \"Write a creative story about a robot who discovers emotions.\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "temperatures = [0.3, 0.7, 1.2]\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = basic_inference(model, tokenizer, prompt, temperature=temp)\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Local Models vs. API-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_local_vs_api():\n",
    "    \"\"\"Print a comparison between local models and API-based models\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOCAL MODELS VS. API-BASED MODELS\".center(80))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    "┌─────────────────────┬─────────────────────────┬─────────────────────────┐\n",
    "│                     │ Local Models             │ API-Based Models        │\n",
    "├─────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Cost                │ One-time hardware cost   │ Pay per token/request   │\n",
    "│ Privacy             │ Data stays on device     │ Data sent to servers    │\n",
    "│ Setup Complexity    │ Higher                   │ Lower                   │\n",
    "│ Maintenance         │ Manual updates needed    │ Automatic updates       │\n",
    "│ Performance         │ Depends on hardware      │ Consistent              │\n",
    "│ Customization       │ Full control             │ Limited by API          │\n",
    "│ Scaling             │ Limited by hardware      │ Easy to scale           │\n",
    "│ Offline Usage       │ Yes                      │ No                      │\n",
    "│ Model Size Options  │ Limited by hardware      │ Wide range available    │\n",
    "│ Latency             │ Lower (no network)       │ Higher (network delay)  │\n",
    "└─────────────────────┴─────────────────────────┴─────────────────────────┘\n",
    "\n",
    "When to use local models:\n",
    "- Privacy-sensitive applications\n",
    "- Offline environments\n",
    "- Cost-sensitive long-running applications\n",
    "- When you need full control over the model\n",
    "\n",
    "When to use API-based models:\n",
    "- Quick prototyping\n",
    "- Limited local hardware\n",
    "- Need for state-of-the-art large models\n",
    "- Simplicity is prioritized over customization\n",
    "    \"\"\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "compare_local_vs_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating a Simple Gradio Interface (Optional)\n",
    "\n",
    "If you want to create a user interface for your model, you can use Gradio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import gradio as gr\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create a simple Gradio interface for the model\"\"\"\n",
    "    def generate(prompt, temperature=0.7, max_length=512):\n",
    "        return basic_inference(model, tokenizer, prompt, max_length=max_length, temperature=temperature)\n",
    "    \n",
    "    demo = gr.Interface(\n",
    "        fn=generate,\n",
    "        inputs=[\n",
    "            gr.Textbox(lines=4, placeholder=\"Enter your prompt here...\", label=\"Prompt\"),\n",
    "            gr.Slider(0.1, 1.5, value=0.7, label=\"Temperature\"),\n",
    "            gr.Slider(64, 1024, value=512, step=64, label=\"Max Length\")\n",
    "        ],\n",
    "        outputs=gr.Textbox(label=\"Generated Text\"),\n",
    "        title=\"Local LLM Demo\",\n",
    "        description=\"Generate text using a local language model\"\n",
    "    )\n",
    "    return demo\n",
    "\n",
    "# Uncomment to create and launch the interface\n",
    "# interface = create_gradio_interface()\n",
    "# interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. Hugging Face's ecosystem and its components\n",
    "2. How to set up the environment for local models\n",
    "3. Loading a model from Hugging Face Hub\n",
    "4. Performing basic inference with the model\n",
    "5. Experimenting with different generation parameters\n",
    "6. Comparing local models with API-based models\n",
    "\n",
    "Next steps:\n",
    "- Try different models from the Hugging Face Hub\n",
    "- Experiment with fine-tuning models on your own data\n",
    "- Explore more advanced inference parameters\n",
    "- Integrate local models into your applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
